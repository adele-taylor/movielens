---
title: "Movielens Project"
author: "Adele Taylor"
date: "09/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                            title = as.character(title),
                                            genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
      semi_join(edx, by = "movieId") %>%
      semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

options(digits=4)
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
library(knitr)
library(kableExtra)
```

## Introduction

MovieLens (movielens.org) is a non-commercial film rating and recommendation website run by the GroupLens research group at the University of Minnesota. The aim of this project is to use the 10M dataset to design and validate a movie recommendation system.

The dataset was initially divided into training and validation sets using code provided by the edX course "HarvardX PH125.9x Data Science: Capstone". All investigations and analysis was performed on the training set ("edx"), which comprised over 9 million observations consisting of the variables "userId", "movieId", "timestamp", "title" and "genres". All further reference to "the dataset" refer to this subset, unless explicitly stated otherwise.

We will briefly examine the dataset to gain insights which may be useful, before trying out a range of models on a smaller subset of the dataset. We will construct a regularised linear model and use this and other potentially well-performing models to create an ensemble which we will use to predict ratings for the validation dataset.

All the code for the analyses mentioned in this report is included in the ataylor_movielens.R file.

## Analysis

The dataset contains ratings for `r length(unique(edx$movieId)) %>% print()` movies by `r length(unique(edx$userId)) %>% print()' users. To get an idea of how these ratings are distributed, several graphs were created as below.


```{r analysis_1, echo=FALSE, message=FALSE}
median_user_reviews <- edx %>% group_by(userId) %>% summarise(n=n()) %>% pull() %>% median()
edx %>% group_by(userId) %>% summarise(n=n()) %>% ggplot(aes(n))+geom_histogram()+geom_vline(aes(xintercept=median_user_reviews, colour="Median"))+ggtitle("Number of reviews per user")+xlab("")+ylab("")+scale_color_manual(name = "", values = c(Median = "red")) 

median_film_reviews <- edx %>% group_by(movieId) %>% summarise(n=n()) %>% pull() %>% median()
edx %>% group_by(movieId) %>% summarise(n=n()) %>% ggplot(aes(n))+geom_histogram()+geom_vline(aes(xintercept = median_film_reviews, colour="Median"))+ggtitle("Number of reviews per film")+xlab("")+ylab("")+scale_colour_manual(name="", values=c(Median="red"))

edx %>% group_by(movieId) %>% summarise(n=n(), average=mean(rating)) %>% mutate(number=ifelse(n<10, "<10 reviews", ">=10 reviews")) %>% ggplot(aes(n, average, col=number))+geom_point()+ggtitle("Average rating vs number of reviews per film")+scale_colour_manual(name="", breaks=c('<10 reviews'), values=scales::hue_pal()(2))

temp <- edx %>% group_by(movieId) %>% summarise(n=n(), average=mean(rating))
```

It's clear that there is a large variation in the number of reviews a film gets and in the number of ratings a given user makes. We can also see that there is a slight positive correlation (r=`r cor(temp$n, temp$average)') between the number of reviews a film gets and its average (mean) rating. 

To investigate further which predictors we will train and then test several simple models using randomly generated subsets. The sheer size of the dataset will prevent us from using all of it to train models, but we will use a much larger subset when constructing the final ensemble.

We will use the caret package and train a general linear model ("glm"), a k-nearest neighbours model ("knn") as well as models using random partitioning ("rpart") and the popular Random Forest algorithm ("rf"). First we will only use the userId, then only the movieId, and finally both.


```{r analysis_2, echo=FALSE, message=FALSE, warnings=FALSE}
set.seed(20, sample.kind="Rounding")
subset <- edx[sample(9000055, 10000),]

set.seed(10, sample.kind="Rounding")
subset2 <- edx %>% semi_join(subset, by="userId")
subset2 <- subset2[sample(nrow(subset2),10000),]


#going to look at four basic models to start with
models <- c("glm", "knn", "rf", "rpart")

RMSE <- function(real, predicted){
sqrt(mean((real-predicted)^2))
}

set.seed(3, sample.kind="Rounding")

#frst userId alone
userId_models <- sapply(models,function(model){
train(rating ~ userId, model=model, data=subset)
})
userId_model_predictions <- sapply(userId_models["finalModel",], function(model){
  predict(model,subset2)
  })

userId_rmses <- apply(userId_model_predictions, 2, function(predictions){RMSE(predictions,subset2$rating)})


#then movieId alone
set.seed(1, sample.kind="Rounding")
subset3 <- edx[sample(9000055, 10000),]

set.seed(8, sample.kind="Rounding")
subset4 <- edx %>% semi_join(subset3, by="movieId")
subset4 <- subset4[sample(nrow(subset4),10000),]

set.seed(4, sample.kind="Rounding")

movieId_models <- sapply(models,function(model){
  train(rating ~ movieId, model=model, data=subset3)
})
movieId_model_predictions <- sapply(movieId_models["finalModel",], function(model){
  predict(model,subset4)
})

movieId_rmses <- apply(movieId_model_predictions, 2, function(predictions){RMSE(predictions.,subset4$rating)})


#finally both

set.seed(19, sample.kind="Rounding")
subset5 <- edx[sample(9000055, 10000),]

set.seed(80, sample.kind="Rounding")
subset6 <- edx %>% semi_join(subset5, by="movieId")
subset6 <- subset6[sample(nrow(subset6),10000),]

set.seed(1, sample.kind="Rounding")

user_and_movie_models <- sapply(models,function(model){
  train(rating ~ userId + movieId, model=model, data=subset5)
})
user_and_movie_model_predictions <- sapply(user_and_movie_models["finalModel",], function(model){
  predict(model,subset6)
})

user_and_movie_rmses <- apply(userId_model_predictions,2, function(predictions){RMSE(predictions,subset6$rating)})

predictions <- userId_rmses %>% rbind(movieId_rmses) %>% rbind(user_and_movie_rmses)
rownames(predictions) <- c("userId only", "movieId only", "both userId and movieId")
predictions %>% kable() %>% kable_styling()

```

None of these models produce particularly good root mean square errors - although this is probably due to the small size of the subsets used for training (~0.1% of the dataset) as much as whether a particular model is a good fit for this task. As there is little difference between them, we will use all in our final ensemble. 

The regularised linear model is built from taking the mean rating and adding effects for the user and movie seperately (by generating the user/movie average and taking the difference from global mean with a factor proportional to 1/n to reduce impact of users/movies with few ratings). We check the potential for this model first by partitioning the dataset into training/test subsets. Unlike with the other models, we can use a much larger proportion of the full dataset for training, which should help.

In order to tune this model, we tested a number of different parameters $\lambda$ and take the minimum.

```{r analysis_3, echo=F, message=F, warning=F}
set.seed(23, sample.kind="Rounding")
test_index <- edx %>% createDataPartition(times = 1, p=0.1)
test_set <- edx[index]
train_set <- edx[-index] %>% semi_join(test_set, by="userId") %>% semi_join(test_set, by="movieId")

###tune a regularised linear model

lambdas <- seq(0, 10, 0.25)

regularised_model <- function(l, validation_set){

  mu <- mean(train_set$rating)

  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))

  b_u <- train_set %>%
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))

  predicted_ratings <-
    validation_set %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)

  return(predicted_ratings)
}

regularised_model_RMSE <- sapply(lambdas, function(l){
  RMSE(regularised_model(l),test_set$rating)
})

plot(lambdas,regularised_model_RMSE)
l<- lambdas[which(regularised_model_RMSE==min(regularised_model_RMSE))]
min(regularised_model_RMSE)
```

From the graph we can see that using $\lambda = $ `r l' gives us a root mean square error of `r min(regularised)', which is much better than the previous models.

We will now proceed with building an ensemble, using the average of the previously generated models (with $\lambda = $ `r l' for the regularised linear model). We show the R code for this model below.

```{r final_model, message=F, warnings=F}

final_model <- function(validation_set){
  user <- sapply(userId_models["finalModel",], function(model){
    predict(model, validation_set)
  }) %>% rowMeans()
  movie <- sapply(movieId_models["finalModel"], function(model){
    predict(model, validation_set)
  }) %>% rowMeans()
  user_and_movie <- sapply(user_and_movie_models["finalModel"], function(model){
    predict(model, validation_set)
  }) %>% rowMeans()
  regularised <- regularised_model(l, validation_set)
  
  return((user+movie+user_and_movie+regularised)/4)
  
}


```



## Results

```{r results, echo=F, message=F, warnings=F}

final_predictions <- final_model(validation)

final_rmse <- RMSE(final_predictions, validation$rating)

```

Using this final model on the validation set gave a final RMSE of `r final_rmse`. 

## Conclusion


Due to time and computer power constraints, we could only perform a subset of the analysis and training of potential models that we would have wished. In particular, we did not fully investigate whether adding some (or all) of the genre tags as predictors would have increased accuracy or provided any additonal information beyond what we can glean from user and movie effects. To do this, we would first have to introduce extra variables/columns (one for each genre tag) and then investigate fully how these may interact with the other predictors. I suspect by themselves they would not provide any additional information beyond the movie effect, but the interaction with the user may yield better results. 

Retraining the chosen models on the whole of the edx dataset would almost certainly have improved the performance of the final ensemble, but again this was unable to be completed due to time constraints.